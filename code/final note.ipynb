{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqzIw19IPauF"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vhc1c80GSmb"
      },
      "outputs": [],
      "source": [
        "!pip install dill"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8LbCucUh6h8"
      },
      "outputs": [],
      "source": [
        "pip install stow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_nh0ZWfh6uE"
      },
      "outputs": [],
      "source": [
        "pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYZhudmJ4Z5R"
      },
      "outputs": [],
      "source": [
        "pip install utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wlcBkDUCH4Ue"
      },
      "outputs": [],
      "source": [
        "pip install mltu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WDh6ZbIwz7bf"
      },
      "outputs": [],
      "source": [
        "pip install tf2onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEGX-6cm5OiZ"
      },
      "outputs": [],
      "source": [
        "pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FucsVK4QLm0F"
      },
      "outputs": [],
      "source": [
        "import stow\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "from urllib.request import urlopen\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "from mltu.configs import BaseModelConfigs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ4XS24AzMMp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
        "except: pass\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "\n",
        "from mltu.preprocessors import ImageReader\n",
        "from mltu.transformers import ImageResizer, LabelIndexer, LabelPadding, ImageShowCV2\n",
        "from mltu.augmentors import RandomBrightness, RandomRotate, RandomErodeDilate, RandomSharpen\n",
        "from mltu.annotations.images import CVImage\n",
        "\n",
        "from mltu.tensorflow.dataProvider import DataProvider\n",
        "from mltu.tensorflow.losses import CTCloss\n",
        "from mltu.tensorflow.callbacks import Model2onnx, TrainLogger\n",
        "from mltu.tensorflow.metrics import CWERMetric\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "from urllib.request import urlopen\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5hMc5-9-QJX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF_IUIyt8eG_"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image=cv2.imread(image)\n",
        "\n",
        "    # Apply Gaussian blur to reduce noise\n",
        "    blurred = cv2.GaussianBlur(image, (7,7), 0)\n",
        "\n",
        "    #Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    return gray\n",
        "\n",
        "def threshold_image(image):\n",
        "    # Apply Otsu's method to get the initial threshold\n",
        "    _, otsu_thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Apply Gaussian Adaptive Thresholding using the Otsu threshold as the base\n",
        "    gaussian_thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 91, 2)\n",
        "\n",
        "    # Combine the Otsu and Gaussian Adaptive Thresholding results\n",
        "    combined_thresh = cv2.bitwise_and(otsu_thresh, gaussian_thresh)\n",
        "\n",
        "    return combined_thresh\n",
        "\n",
        "def perform_morphological_operations(image):\n",
        "    # Define a structuring element (kernel) for the morphological operations\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,5))\n",
        "\n",
        "    # Perform opening (erosion followed by dilation) to remove small noise\n",
        "    opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
        "    return opening\n",
        "\n",
        "def adjust_images(images_path, brightness=20, contrast=1.2):\n",
        "\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "    # Convert the image to grayscale\n",
        "    #gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Adjust the brightness and contrast\n",
        "    adjusted = np.clip(image * contrast + brightness, 0, 255).astype(np.uint8)\n",
        "    return adjusted\n",
        "\n",
        "def process_image(image):\n",
        "    # Read the image\n",
        "    # image = cv2.imread(image_path)\n",
        "\n",
        "    # Check if the image is loaded successfully\n",
        "    if image is None:\n",
        "        print(\"Error: Failed to load image from\", image)\n",
        "        return\n",
        "\n",
        "    # Adjust the brightness and contrast\n",
        "    #adjusted = adjust_images(image)\n",
        "\n",
        "    # Preprocess the adjusted image\n",
        "    preprocessed = preprocess_image(image)\n",
        "\n",
        "    # Threshold the preprocessed image\n",
        "    thresholded = threshold_image(preprocessed)\n",
        "\n",
        "    # Perform morphological operations on the thresholded image\n",
        "    morph_processed = perform_morphological_operations(thresholded)\n",
        "\n",
        "    # Invert the image to get a white background with black text\n",
        "    result = cv2.bitwise_not(morph_processed)\n",
        "\n",
        "    return result\n",
        "    # Display the original image and the processed results\n",
        "    # cv2_imshow(image)\n",
        "    # #cv2_imshow(adjusted)\n",
        "    # cv2_imshow(preprocessed)\n",
        "    # cv2_imshow(thresholded)\n",
        "    # cv2_imshow(morph_processed)\n",
        "    # cv2_imshow(result)\n",
        "\n",
        "    # cv2.waitKey(0)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# image_path = \"/content/Screenshot 2024-05-05 214227.png\"  # Replace with your image path\n",
        "# img=process_image(image_path)\n",
        "# cv2.imwrite(\"image2.jpg\",img)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HAfG1Nt9dhU"
      },
      "outputs": [],
      "source": [
        "from skimage.filters import threshold_otsu\n",
        "import numpy as np\n",
        "import cv2\n",
        "img = cv2.imread(\"/content/testo4.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-PWkOaHf4mQ"
      },
      "outputs": [],
      "source": [
        "# Dictionary of contextual corrections with lists instead of sets\n",
        "contextual_corrections = {\n",
        "    \"their\": [\"thier\"],\n",
        "    \"there\": [\"ther\"],\n",
        "    \"they\": [\"thay\"],\n",
        "    \"tter\": [\"other\"],\n",
        "    \"your\": [\"yor\"],\n",
        "    \"8us\": [\"DBMS\"],\n",
        "    \"oasics\": [\"Basics\"],\n",
        "    \"DBus\": [\"DBMS\"],\n",
        "    \"effction\":[\"collection\"],\n",
        "    \"pors\": [\"DBMS\"],\n",
        "    \"cbection\": [\"collection\"],\n",
        "    \"togtter\": [\"together\"],\n",
        "    \"Er\": [\"EX\"],\n",
        "    \"-\": [\":\"],\n",
        "    \"Pecnbsy\": [\"Technology\"],\n",
        "    \"erice\": [\"component\"],\n",
        "    \"Intertae\": [\"Interface\"],\n",
        "    \"ard\": [\"card\"],\n",
        "    \"hat\": [\"hub\"],\n",
        "    \"Pouter\": [\"Router\"],\n",
        "    \"ro\": [\"hub\"],\n",
        "    \"intoration\": [\"information\"],\n",
        "    \"info\": [\"information\"],\n",
        "    \"comects\": [\"connects\"],\n",
        "    \"tat\": [\"hub\"],\n",
        "    \"t\": [\"A\"],\n",
        "    \"bost\": [\"host\"],\n",
        "    \"al\": [\"all\"],\n",
        "    \"itornation\": [\"information\"],\n",
        "    \"lerme\": [\"consume\"],\n",
        "    \"lar\": [\"consume\"],\n",
        "    \"bretwark\": [\"Network\"],\n",
        "    \"crerr--r\": [\"Operating\"],\n",
        "    \"ere\": [\"System\"],\n",
        "    \"Tracess\": [\"process\"],\n",
        "    \"rre\": [\"state\"],\n",
        "    \"rurrinl\": [\"running\"],\n",
        "    \"tlock\": [\"block\"],\n",
        "    \"Tearc\": [\"Device\"],\n",
        "    \"aece\": [\"queue\"],\n",
        "    \"gvene\": [\"queue\"],\n",
        "    \"devile\": [\"device\"],\n",
        "    \"cf\": [\"CPU\"],\n",
        "    \"rmput\": [\"inpuy\"],\n",
        "    \"Seel\": [\"Ready\"],\n",
        "    \"arece\": [\"queue\"],\n",
        "    \"M\": [\"AI\"],\n",
        "    \"hibern\": [\"hidden\"],\n",
        "    \"apenalr\": [\"process\"],\n",
        "    \"eaa\": [\"operations\"],\n",
        "    \"ter\": [\"system\"],\n",
        "    \"frece\": [\"queue\"],\n",
        "    \"scr\": [\"load\"],\n",
        "    \"-rr\": [\"in\"],\n",
        "    \"34ste\": [\"system\"],\n",
        "    \"cear\": [\"ready\"],\n",
        "    \"sacce\": [\"queue\"],\n",
        "    \"quene\": [\"queue\"],\n",
        "    \"cP\": [\"CPU\"],\n",
        "    \"Ereurrce\": [\"Device\"],\n",
        "    \"crecc\": [\"queue\"],\n",
        "    \"areie\": [\"queue\"],\n",
        "    \"d\": [\"of\"],\n",
        "    \"alr\": [\"processes\"],\n",
        "    \"proaesiystied  \"\"\": [\"need to load in system\"],\n",
        "    \"i-\": [\"Introduction\"],\n",
        "    \"--e S -\": [\"Data\"],\n",
        "    '\" S paietanf , Fatsthantecantrnston': [\"is a colection of facts that can be processed to produce information\"],\n",
        "    \"-slse mangenacsystens-\": [\"Database Management System\"],\n",
        "    \"iii\": [\"database MS is software that allows computers to perform database function of storing, receiving, adding and modify data\"],\n",
        "    \"amahataanaltres\": [\"Social data analysis\"],\n",
        "    \"3I Component . slated -unetwort that are Comcted\": [\"1. components: isolated subnetwork that are connected\"],\n",
        "    \"dscomacted\": [\"disconnected\"],\n",
        "    \"ttween\": [\"between\"],\n",
        "    \"raaturort\": [\"networks\"],\n",
        "    \"I\": [\"of\"],\n",
        "    \"nodo\": [\"nodes\"],\n",
        "    \"toeack\": [\"to each\"],\n",
        "    \"ter\": [\"other\"],\n",
        "    \"conacteel\": [\"connected\"],\n",
        "    \"ooer\": [\"other\"],\n",
        "    \"groo\": [\"group\"],\n",
        "    \"everaise-\":[\"Bridge\"],\n",
        "    \"wodc\": [\"node\"],\n",
        "    \"reatnwnen\": [\"that\"],\n",
        "    \"sumond\": [\"when\"],\n",
        "    \"ranale\": [\"removed\"],\n",
        "    \"an\": [\"it\"],\n",
        "    \"ded\": [\"create\"],\n",
        "    \"f\": [\"connected\"],\n",
        "    \"3I\": [\"3.\"],\n",
        "    \"Pameter\": [\"Diameter\"],\n",
        "    \"I\": [\"of\"],\n",
        "    \"o\": [\"of\"],\n",
        "    \"shotewats\": [\"shortest path\"],\n",
        "    \"3S\": [\"3.\"],\n",
        "    \"lonsit\": [\"long it\"],\n",
        "    \"paiihakieguoriseruaton\": [\"will take for information\"],\n",
        "    \"I mos eonsit aibl iken oresiveruaton\": [\"to pass through network\"],\n",
        "    \"shotatwatt\": [\"shortest path\"],\n",
        "    \"Disit\": [\"Disk\"],\n",
        "    \"blacticare\": [\"blocks are\"],\n",
        "    \"rad\": [\"read\"],\n",
        "    \"-n\": [\"on\"],\n",
        "    \",gntsret inmasoutter\": [\"and put into buffer\"],\n",
        "    \"crlaisariented itarase Lenaucas ctrmig\": [\"pool which is made by buffer manager\"],\n",
        "    \"f\": [\"of\"],\n",
        "    \"sex\": [\"2.\"],\n",
        "    \"or\": [\"cons:\"],\n",
        "    \"iost\": [\"cost\"],\n",
        "    \"icost\": [\"cost\"],\n",
        "    \"deltirng\": [\"deleting\"],\n",
        "    \"deletirny\": [\"deleting\"],\n",
        "    \"Tr\": [\"pros:\"],\n",
        "    \"'s Rdua \": [\"1.cost\"],\n",
        "    \"ro\": [\"IO\"],\n",
        "    '\"t': [\"if\"],\n",
        "    \"altnbute\": [\"attributes\"],\n",
        "    \"ex\": [\"1.\"],\n",
        "    \"Zmprore\": [\"Improve\"],\n",
        "    \"cacke\": [\"cache\"],\n",
        "    \"pretormnce\": [\"performance\"],\n",
        "    \"ss\": [\"2.\"],\n",
        "    \"Emprore\": [\"Improve\"],\n",
        "    \"fret torr Eemutrt\": [\" Network Benefit:\"]\n",
        "}\n",
        "\n",
        "def contextual_analysis_correction(text):\n",
        "    words = text.split()\n",
        "    corrected_words = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        word = words[i]\n",
        "        word_lower = word.lower()\n",
        "\n",
        "        if word_lower in contextual_corrections:\n",
        "            # Check if previous or next word matches any of the allowed corrections\n",
        "            prev_word = words[i - 1].lower() if i > 0 else \"\"\n",
        "            next_word = words[i + 1].lower() if i < len(words) - 1 else \"\"\n",
        "\n",
        "            if prev_word in contextual_corrections[word_lower]:\n",
        "                corrected_words.append(word)  # Keep original word if contextually correct\n",
        "            elif next_word in contextual_corrections[word_lower]:\n",
        "                corrected_words.append(word)  # Keep original word if contextually correct\n",
        "            else:\n",
        "                # Apply correction logic based on context\n",
        "                corrected_words.append(contextual_corrections[word_lower][0])  # Use first suggestion\n",
        "        else:\n",
        "            corrected_words.append(word)  # Keep original word if not in dictionary\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return \" \".join(corrected_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35VSC_iBB6oE"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Initialize spell checker\n",
        "spell_checker = SpellChecker()\n",
        "\n",
        "# Function for spell checking and correction\n",
        "def correct_spelling(text):\n",
        "    # Split text into words and correct each word\n",
        "    corrected_text = []\n",
        "    for word in text.split():\n",
        "        corrected_word = spell_checker.correction(word)\n",
        "        if corrected_word is not None:\n",
        "            corrected_text.append(corrected_word)\n",
        "        else:\n",
        "            corrected_text.append(word)  # Keep original word if no correction found\n",
        "    return ' '.join(corrected_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sU1mLWpi_wC"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import dill\n",
        "# Path to the saved fine-tuned model checkpoint\n",
        "checkpoint_path = '/content/drive/MyDrive/Kaggle-runs/train_backup/weights/best.pt'\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = YOLO(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SU5HRf6jCYA",
        "outputId": "2d0236b6-57ee-42ae-963d-020503198fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 704x544 1 Figure, 1 Headline, 3 Nones, 3 Subtitles, 3810.6ms\n",
            "Speed: 5.4ms preprocess, 3810.6ms inference, 1.1ms postprocess per image at shape (1, 3, 704, 544)\n",
            "Detected image saved to: /content/drive/MyDrive/saved_yolo/detected_image.jpg\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 1s 905ms/step\n",
            "1/1 [==============================] - 1s 916ms/step\n",
            "1/1 [==============================] - 1s 924ms/step\n",
            "1/1 [==============================] - 1s 537ms/step\n",
            "1/1 [==============================] - 0s 496ms/step\n",
            "1/1 [==============================] - 0s 498ms/step\n",
            "1/1 [==============================] - 1s 502ms/step\n",
            "1/1 [==============================] - 0s 486ms/step\n",
            "1/1 [==============================] - 0s 499ms/step\n",
            "1/1 [==============================] - 1s 855ms/step\n",
            "1/1 [==============================] - 1s 903ms/step\n",
            "1/1 [==============================] - 1s 969ms/step\n",
            "<h1>Operating System</h1>\n",
            "<u>traces state</u>\n",
            "<p>ready , running block</p>\n",
            "<u>tear queue</u>\n",
            "<p></p>\n",
            "<p>queue set of processes in block wait</p>\n",
            "<p>for input output device</p>\n",
            "<p>seaweed car at procser-n memory</p>\n",
            "<p>waiting to enter cup</p>\n",
            "<p></p>\n",
            "<p>queue set of processes in block wait</p>\n",
            "<p>for input output device</p>\n",
            "<u>Seel queue</u>\n",
            "<img src=\"/content/drive/MyDrive/saved_yolo/figure_67_625_722_1009.jpg\" alt=\"Figure\"/>\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "from mltu.utils.text_utils import ctc_decoder, get_cer, get_wer\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "# Test the model on an image (assuming 'model' and 'img' are defined)\n",
        "results = model(img)\n",
        "save_dir = '/content/drive/MyDrive/saved_yolo'\n",
        "\n",
        "# upload pretrained ocr model\n",
        "configs = BaseModelConfigs.load(\"/content/drive/MyDrive/202301131202/configs.yaml\")\n",
        "model = load_model('/content/drive/MyDrive/202301131202/model.h5', custom_objects={'CTCloss': CTCloss}, compile=False)\n",
        "\n",
        "# Create a directory to save the detected images if it doesn't exist\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Initialize a list to hold the HTML content\n",
        "html_content = []\n",
        "\n",
        "\n",
        "\n",
        "# Iterate over results to extract bounding boxes and apply OCR\n",
        "for idx, result in enumerate(results):\n",
        "    # Save the detected image\n",
        "    output_image_path = os.path.join(save_dir, f'detected_image.jpg')\n",
        "    result.save(output_image_path)\n",
        "    print(f\"Detected image saved to: {output_image_path}\")\n",
        "    boxes = result.boxes\n",
        "\n",
        "    sorted_indices = torch.argsort(boxes.xyxy[:, 1])\n",
        "\n",
        "    # Sort both xyxy and cls tensors\n",
        "    xyxy_sorted = boxes.xyxy[sorted_indices]\n",
        "    cls_sorted = boxes.cls[sorted_indices]\n",
        "    class_sort=[]\n",
        "    for element in cls_sorted:\n",
        "        class_sort.append(element)\n",
        "    for idx,detection in enumerate(xyxy_sorted):\n",
        "        # Extract bounding box coordinates\n",
        "        x1, y1, x2, y2 = map(int, detection.tolist())\n",
        "\n",
        "        # Crop the image based on bounding box coordinates\n",
        "        cropped_img = img[y1:y2, x1:x2]\n",
        "\n",
        "        # Extract the label (class name)\n",
        "        label = class_sort[idx].item()\n",
        "        class_name = result.names[label]\n",
        "\n",
        "        # Determine the HTML tag based on the class name\n",
        "        if class_name == 'Headline':\n",
        "            # Apply OCR to the cropped image using the model\n",
        "            image=process_image(cropped_img)\n",
        "            # cv2_imshow(cropped_img)\n",
        "            image = cv2.resize(cropped_img, (1408, 96))\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = np.expand_dims(image, axis=0)\n",
        "            prediction_text = model.predict(image)[0]\n",
        "            prediction_text = np.array([prediction_text])\n",
        "            text = ctc_decoder(prediction_text,configs.vocab)\n",
        "            corrected_text = contextual_analysis_correction(text[0])\n",
        "            corrected = correct_spelling(corrected_text)\n",
        "            html_tag = 'h1'\n",
        "            html_content.append(f'<{html_tag}>{corrected}</{html_tag}>')\n",
        "        elif class_name == 'Subtitle':\n",
        "            # cv2_imshow(cropped_img)\n",
        "            # Apply OCR to the cropped image using the model\n",
        "            image=process_image(cropped_img)\n",
        "            image = cv2.resize(cropped_img, (1408, 96))\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = np.expand_dims(image, axis=0)\n",
        "            prediction_text = model.predict(image)[0]\n",
        "            prediction_text = np.array([prediction_text])\n",
        "            text = ctc_decoder(prediction_text,configs.vocab)\n",
        "            corrected_text = contextual_analysis_correction(text[0])\n",
        "            corrected = correct_spelling(corrected_text)\n",
        "            # Use underline tag for subtitle\n",
        "            html_content.append(f'<u>{corrected}</u>')\n",
        "        elif class_name == 'Figure':\n",
        "            # Save the cropped image and create an img tag\n",
        "            figure_path = os.path.join(save_dir, f'figure_{x1}_{y1}_{x2}_{y2}.jpg')\n",
        "            cv2.imwrite(figure_path, cropped_img)\n",
        "            html_content.append(f'<img src=\"{figure_path}\" alt=\"Figure\"/>')\n",
        "        else:\n",
        "            text_image=process_image(cropped_img)\n",
        "            # cv2_imshow(cropped_img)\n",
        "            img_gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n",
        "            ret, thresh2 = cv2.threshold(img_gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
        "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (150,2))\n",
        "            mask = cv2.morphologyEx(thresh2, cv2.MORPH_DILATE, kernel)\n",
        "            bboxes = []\n",
        "            bboxes_img = cropped_img.copy()\n",
        "            contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            contours = contours[0] if len(contours) == 2 else contours[1]\n",
        "            for cntr in contours:\n",
        "                x,y,w,h = cv2.boundingRect(cntr)\n",
        "                cv2.rectangle(bboxes_img, (x, y), (x+w, y+h), (0,0,255), 1)\n",
        "                bboxes.append((x,y,w,h))\n",
        "\n",
        "            # cv2_imshow(bboxes_img)\n",
        "            # # print(bboxes)\n",
        "            for bbox in reversed(bboxes):\n",
        "                x, y, w, h = bbox\n",
        "                roi = cropped_img[y:y+h, x:x+w]  # Extract region of interest (ROI)\n",
        "\n",
        "                # cv2_imshow(roi)\n",
        "                image = cv2.resize(roi, (1408, 96))\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = np.expand_dims(image, axis=0)\n",
        "                prediction_text = model.predict(image)[0]\n",
        "                prediction_text = np.array([prediction_text])\n",
        "                text = ctc_decoder(prediction_text,configs.vocab)\n",
        "                corrected_text = contextual_analysis_correction(text[0])\n",
        "                corrected = correct_spelling(corrected_text)\n",
        "                html_content.append(f'<p>{corrected}</p>')\n",
        "\n",
        "# Combine the HTML content into a single string\n",
        "html_output = '\\n'.join(html_content)\n",
        "\n",
        "# Output the HTML content\n",
        "print(html_output)\n",
        "\n",
        "# Optionally, save the HTML output to a file\n",
        "with open('output.html', 'w') as file:\n",
        "    file.write(html_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMJLzL_ngMWg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}